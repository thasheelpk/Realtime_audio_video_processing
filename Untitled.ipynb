{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092\n",
      "8763\n",
      "32\n",
      "40460\n",
      "40460\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 299, 299, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_658 (Conv2D)             (None, 149, 149, 32) 864         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 149, 149, 32) 96          conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, 149, 149, 32) 0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_659 (Conv2D)             (None, 147, 147, 32) 9216        activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 147, 147, 32) 96          conv2d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_659 (Activation)     (None, 147, 147, 32) 0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 147, 147, 64) 18432       activation_659[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 147, 147, 64) 192         conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_660 (Activation)     (None, 147, 147, 64) 0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 73, 73, 64)   0           activation_660[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 73, 73, 80)   5120        max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 73, 73, 80)   240         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_661 (Activation)     (None, 73, 73, 80)   0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 71, 71, 192)  138240      activation_661[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 71, 71, 192)  576         conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_662 (Activation)     (None, 71, 71, 192)  0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 35, 35, 192)  0           activation_662[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 35, 35, 64)   192         conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_666 (Activation)     (None, 35, 35, 64)   0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 35, 35, 96)   55296       activation_666[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 35, 35, 48)   144         conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 35, 35, 96)   288         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_664 (Activation)     (None, 35, 35, 48)   0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_667 (Activation)     (None, 35, 35, 96)   0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_63 (AveragePo (None, 35, 35, 192)  0           max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 35, 35, 64)   76800       activation_664[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 35, 35, 96)   82944       activation_667[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_63[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 35, 35, 64)   192         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 35, 35, 64)   192         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 35, 35, 96)   288         conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 35, 35, 32)   96          conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_663 (Activation)     (None, 35, 35, 64)   0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_665 (Activation)     (None, 35, 35, 64)   0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_668 (Activation)     (None, 35, 35, 96)   0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_669 (Activation)     (None, 35, 35, 32)   0           batch_normalization_669[0][0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_663[0][0]             \n",
      "                                                                 activation_665[0][0]             \n",
      "                                                                 activation_668[0][0]             \n",
      "                                                                 activation_669[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, 35, 35, 64)   192         conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, 35, 35, 64)   0           batch_normalization_673[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 35, 35, 96)   55296       activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 35, 35, 48)   144         conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, 35, 35, 96)   288         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, 35, 35, 48)   0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, 35, 35, 96)   0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_64 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 35, 35, 64)   76800       activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 35, 35, 96)   82944       activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_64[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 35, 35, 64)   192         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, 35, 35, 64)   192         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, 35, 35, 96)   288         conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, 35, 35, 64)   192         conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_670 (Activation)     (None, 35, 35, 64)   0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, 35, 35, 64)   0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_675 (Activation)     (None, 35, 35, 96)   0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_676 (Activation)     (None, 35, 35, 64)   0           batch_normalization_676[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_670[0][0]             \n",
      "                                                                 activation_672[0][0]             \n",
      "                                                                 activation_675[0][0]             \n",
      "                                                                 activation_676[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, 35, 35, 64)   192         conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_680 (Activation)     (None, 35, 35, 64)   0           batch_normalization_680[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 35, 35, 96)   55296       activation_680[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, 35, 35, 48)   144         conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, 35, 35, 96)   288         conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_678 (Activation)     (None, 35, 35, 48)   0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_681 (Activation)     (None, 35, 35, 96)   0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_65 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 35, 35, 64)   76800       activation_678[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 35, 35, 96)   82944       activation_681[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_65[0][0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, 35, 35, 64)   192         conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, 35, 35, 64)   192         conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, 35, 35, 96)   288         conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, 35, 35, 64)   192         conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_677 (Activation)     (None, 35, 35, 64)   0           batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_679 (Activation)     (None, 35, 35, 64)   0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_682 (Activation)     (None, 35, 35, 96)   0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_683 (Activation)     (None, 35, 35, 64)   0           batch_normalization_683[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_677[0][0]             \n",
      "                                                                 activation_679[0][0]             \n",
      "                                                                 activation_682[0][0]             \n",
      "                                                                 activation_683[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, 35, 35, 64)   192         conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_685 (Activation)     (None, 35, 35, 64)   0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 35, 35, 96)   55296       activation_685[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, 35, 35, 96)   288         conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_686 (Activation)     (None, 35, 35, 96)   0           batch_normalization_686[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 17, 17, 96)   82944       activation_686[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, 17, 17, 384)  1152        conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, 17, 17, 96)   288         conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_684 (Activation)     (None, 17, 17, 384)  0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, 17, 17, 96)   0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D) (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_684[0][0]             \n",
      "                                                                 activation_687[0][0]             \n",
      "                                                                 max_pooling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_692 (BatchN (None, 17, 17, 128)  384         conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_692 (Activation)     (None, 17, 17, 128)  0           batch_normalization_692[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 17, 17, 128)  114688      activation_692[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_693 (BatchN (None, 17, 17, 128)  384         conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_693 (Activation)     (None, 17, 17, 128)  0           batch_normalization_693[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 17, 17, 128)  114688      activation_693[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_689 (BatchN (None, 17, 17, 128)  384         conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_694 (BatchN (None, 17, 17, 128)  384         conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_689 (Activation)     (None, 17, 17, 128)  0           batch_normalization_689[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_694 (Activation)     (None, 17, 17, 128)  0           batch_normalization_694[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 17, 17, 128)  114688      activation_689[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 17, 17, 128)  114688      activation_694[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_690 (BatchN (None, 17, 17, 128)  384         conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_695 (BatchN (None, 17, 17, 128)  384         conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_690 (Activation)     (None, 17, 17, 128)  0           batch_normalization_690[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_695 (Activation)     (None, 17, 17, 128)  0           batch_normalization_695[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_66 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 17, 17, 192)  172032      activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 17, 17, 192)  172032      activation_695[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_66[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, 17, 17, 192)  576         conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, 17, 17, 192)  576         conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_696 (BatchN (None, 17, 17, 192)  576         conv2d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_697 (BatchN (None, 17, 17, 192)  576         conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, 17, 17, 192)  0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_691 (Activation)     (None, 17, 17, 192)  0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_696 (Activation)     (None, 17, 17, 192)  0           batch_normalization_696[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_697 (Activation)     (None, 17, 17, 192)  0           batch_normalization_697[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_688[0][0]             \n",
      "                                                                 activation_691[0][0]             \n",
      "                                                                 activation_696[0][0]             \n",
      "                                                                 activation_697[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_702 (BatchN (None, 17, 17, 160)  480         conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_702 (Activation)     (None, 17, 17, 160)  0           batch_normalization_702[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, 17, 17, 160)  179200      activation_702[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_703 (BatchN (None, 17, 17, 160)  480         conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_703 (Activation)     (None, 17, 17, 160)  0           batch_normalization_703[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, 17, 17, 160)  179200      activation_703[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_699 (BatchN (None, 17, 17, 160)  480         conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_704 (BatchN (None, 17, 17, 160)  480         conv2d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_699 (Activation)     (None, 17, 17, 160)  0           batch_normalization_699[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_704 (Activation)     (None, 17, 17, 160)  0           batch_normalization_704[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, 17, 17, 160)  179200      activation_699[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, 17, 17, 160)  179200      activation_704[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_700 (BatchN (None, 17, 17, 160)  480         conv2d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_705 (BatchN (None, 17, 17, 160)  480         conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_700 (Activation)     (None, 17, 17, 160)  0           batch_normalization_700[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_705 (Activation)     (None, 17, 17, 160)  0           batch_normalization_705[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_67 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_701 (Conv2D)             (None, 17, 17, 192)  215040      activation_700[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, 17, 17, 192)  215040      activation_705[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_67[0][0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_698 (BatchN (None, 17, 17, 192)  576         conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_701 (BatchN (None, 17, 17, 192)  576         conv2d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_706 (BatchN (None, 17, 17, 192)  576         conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_707 (BatchN (None, 17, 17, 192)  576         conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_698 (Activation)     (None, 17, 17, 192)  0           batch_normalization_698[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_701 (Activation)     (None, 17, 17, 192)  0           batch_normalization_701[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_706 (Activation)     (None, 17, 17, 192)  0           batch_normalization_706[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_707 (Activation)     (None, 17, 17, 192)  0           batch_normalization_707[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_698[0][0]             \n",
      "                                                                 activation_701[0][0]             \n",
      "                                                                 activation_706[0][0]             \n",
      "                                                                 activation_707[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_712 (BatchN (None, 17, 17, 160)  480         conv2d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_712 (Activation)     (None, 17, 17, 160)  0           batch_normalization_712[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, 17, 17, 160)  179200      activation_712[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_713 (BatchN (None, 17, 17, 160)  480         conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_713 (Activation)     (None, 17, 17, 160)  0           batch_normalization_713[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, 17, 17, 160)  179200      activation_713[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_709 (BatchN (None, 17, 17, 160)  480         conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_714 (BatchN (None, 17, 17, 160)  480         conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_709 (Activation)     (None, 17, 17, 160)  0           batch_normalization_709[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_714 (Activation)     (None, 17, 17, 160)  0           batch_normalization_714[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, 17, 17, 160)  179200      activation_709[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, 17, 17, 160)  179200      activation_714[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_710 (BatchN (None, 17, 17, 160)  480         conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_715 (BatchN (None, 17, 17, 160)  480         conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_710 (Activation)     (None, 17, 17, 160)  0           batch_normalization_710[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_715 (Activation)     (None, 17, 17, 160)  0           batch_normalization_715[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_68 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, 17, 17, 192)  215040      activation_710[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, 17, 17, 192)  215040      activation_715[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_68[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_708 (BatchN (None, 17, 17, 192)  576         conv2d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_711 (BatchN (None, 17, 17, 192)  576         conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_716 (BatchN (None, 17, 17, 192)  576         conv2d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_717 (BatchN (None, 17, 17, 192)  576         conv2d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_708 (Activation)     (None, 17, 17, 192)  0           batch_normalization_708[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_711 (Activation)     (None, 17, 17, 192)  0           batch_normalization_711[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_716 (Activation)     (None, 17, 17, 192)  0           batch_normalization_716[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_717 (Activation)     (None, 17, 17, 192)  0           batch_normalization_717[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_708[0][0]             \n",
      "                                                                 activation_711[0][0]             \n",
      "                                                                 activation_716[0][0]             \n",
      "                                                                 activation_717[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_722 (BatchN (None, 17, 17, 192)  576         conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 17, 17, 192)  0           batch_normalization_722[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, 17, 17, 192)  258048      activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_723 (BatchN (None, 17, 17, 192)  576         conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 17, 17, 192)  0           batch_normalization_723[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, 17, 17, 192)  258048      activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_719 (BatchN (None, 17, 17, 192)  576         conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_724 (BatchN (None, 17, 17, 192)  576         conv2d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 17, 17, 192)  0           batch_normalization_719[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 17, 17, 192)  0           batch_normalization_724[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_720 (Conv2D)             (None, 17, 17, 192)  258048      activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, 17, 17, 192)  258048      activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_720 (BatchN (None, 17, 17, 192)  576         conv2d_720[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_725 (BatchN (None, 17, 17, 192)  576         conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 17, 17, 192)  0           batch_normalization_720[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 17, 17, 192)  0           batch_normalization_725[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_69 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_721 (Conv2D)             (None, 17, 17, 192)  258048      activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, 17, 17, 192)  258048      activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_69[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_718 (BatchN (None, 17, 17, 192)  576         conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_721 (BatchN (None, 17, 17, 192)  576         conv2d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_726 (BatchN (None, 17, 17, 192)  576         conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_727 (BatchN (None, 17, 17, 192)  576         conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 17, 17, 192)  0           batch_normalization_718[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 17, 17, 192)  0           batch_normalization_721[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 17, 17, 192)  0           batch_normalization_726[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 17, 17, 192)  0           batch_normalization_727[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_718[0][0]             \n",
      "                                                                 activation_721[0][0]             \n",
      "                                                                 activation_726[0][0]             \n",
      "                                                                 activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 17, 17, 192)  576         conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 17, 17, 192)  0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, 17, 17, 192)  258048      activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_731 (BatchN (None, 17, 17, 192)  576         conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 17, 17, 192)  0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, 17, 17, 192)  258048      activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_728 (BatchN (None, 17, 17, 192)  576         conv2d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 17, 17, 192)  576         conv2d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 17, 17, 192)  0           batch_normalization_728[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 17, 17, 192)  0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, 8, 8, 320)    552960      activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, 8, 8, 192)    331776      activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 8, 8, 320)    960         conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 8, 8, 192)    576         conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 8, 8, 320)    0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 8, 8, 192)    0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_729[0][0]             \n",
      "                                                                 activation_733[0][0]             \n",
      "                                                                 max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_738 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_738 (BatchN (None, 8, 8, 448)    1344        conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_738 (Activation)     (None, 8, 8, 448)    0           batch_normalization_738[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_739 (Conv2D)             (None, 8, 8, 384)    1548288     activation_738[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_735 (BatchN (None, 8, 8, 384)    1152        conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_739 (BatchN (None, 8, 8, 384)    1152        conv2d_739[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 8, 8, 384)    0           batch_normalization_735[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_739 (Activation)     (None, 8, 8, 384)    0           batch_normalization_739[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 8, 8, 384)    442368      activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_737 (Conv2D)             (None, 8, 8, 384)    442368      activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_740 (Conv2D)             (None, 8, 8, 384)    442368      activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_741 (Conv2D)             (None, 8, 8, 384)    442368      activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_70 (AveragePo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_736 (BatchN (None, 8, 8, 384)    1152        conv2d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_737 (BatchN (None, 8, 8, 384)    1152        conv2d_737[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_740 (BatchN (None, 8, 8, 384)    1152        conv2d_740[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_741 (BatchN (None, 8, 8, 384)    1152        conv2d_741[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_742 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_70[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_734 (BatchN (None, 8, 8, 320)    960         conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_736 (Activation)     (None, 8, 8, 384)    0           batch_normalization_736[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_737 (Activation)     (None, 8, 8, 384)    0           batch_normalization_737[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_740 (Activation)     (None, 8, 8, 384)    0           batch_normalization_740[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_741 (Activation)     (None, 8, 8, 384)    0           batch_normalization_741[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_742 (BatchN (None, 8, 8, 192)    576         conv2d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 8, 8, 320)    0           batch_normalization_734[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_736[0][0]             \n",
      "                                                                 activation_737[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 768)    0           activation_740[0][0]             \n",
      "                                                                 activation_741[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_742 (Activation)     (None, 8, 8, 192)    0           batch_normalization_742[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_734[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_14[0][0]             \n",
      "                                                                 activation_742[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_747 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_747 (BatchN (None, 8, 8, 448)    1344        conv2d_747[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_747 (Activation)     (None, 8, 8, 448)    0           batch_normalization_747[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_744 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_748 (Conv2D)             (None, 8, 8, 384)    1548288     activation_747[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_744 (BatchN (None, 8, 8, 384)    1152        conv2d_744[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_748 (BatchN (None, 8, 8, 384)    1152        conv2d_748[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_744 (Activation)     (None, 8, 8, 384)    0           batch_normalization_744[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_748 (Activation)     (None, 8, 8, 384)    0           batch_normalization_748[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_745 (Conv2D)             (None, 8, 8, 384)    442368      activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_746 (Conv2D)             (None, 8, 8, 384)    442368      activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_749 (Conv2D)             (None, 8, 8, 384)    442368      activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_750 (Conv2D)             (None, 8, 8, 384)    442368      activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_71 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_743 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_745 (BatchN (None, 8, 8, 384)    1152        conv2d_745[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_746 (BatchN (None, 8, 8, 384)    1152        conv2d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_749 (BatchN (None, 8, 8, 384)    1152        conv2d_749[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 8, 8, 384)    1152        conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_751 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_71[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_743 (BatchN (None, 8, 8, 320)    960         conv2d_743[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_745 (Activation)     (None, 8, 8, 384)    0           batch_normalization_745[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_746 (Activation)     (None, 8, 8, 384)    0           batch_normalization_746[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 8, 8, 384)    0           batch_normalization_749[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 8, 8, 384)    0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 8, 8, 192)    576         conv2d_751[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_743 (Activation)     (None, 8, 8, 320)    0           batch_normalization_743[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_745[0][0]             \n",
      "                                                                 activation_746[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 8, 8, 768)    0           activation_749[0][0]             \n",
      "                                                                 activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_751 (Activation)     (None, 8, 8, 192)    0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_743[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_15[0][0]             \n",
      "                                                                 activation_751[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "preprocessed words 0 ==> 0\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [01:26, 4645.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "200\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 34, 200)      200         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2048)         0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 34, 200)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          524544      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          467968      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256)          0           dense_9[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          65792       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            257         dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,058,761\n",
      "Trainable params: 1,058,761\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (1, 200) and (1652, 200) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c3f287a5a35b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Captioning/data\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mf'caption-model.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m \u001b[0mcaption_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[0;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m   def compile(self,\n",
      "\u001b[1;32mE:\\installation\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[0;32m   1265\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    705\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[0;32m    706\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m   \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3382\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3383\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3384\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3385\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3386\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m    848\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[1;32mE:\\installation\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1115\u001b[0m     \"\"\"\n\u001b[0;32m   1116\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (1, 200) and (1652, 200) are incompatible"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# dhq 2020 Feb 1\n",
    "# Based on Work from Jeff Heaton (T81-558)\n",
    "\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "import tensorflow.keras.applications.mobilenet  \n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow.keras.applications.inception_v3\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow.keras.preprocessing.image\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 6\n",
    "USE_INCEPTION = True\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\"\n",
    "\n",
    "\n",
    "# ### Needed Data\n",
    "# \n",
    "# You will need to download the following data and place it in a folder for this example.  Point the *root_captioning* string at the folder that you are using for the caption generation. This folder should have the following sub-folders.\n",
    "# \n",
    "# * data - Create this directory to hold saved models.\n",
    "# * [glove.6B](https://nlp.stanford.edu/projects/glove/) - Glove embeddings.\n",
    "# * [Flicker8k_Dataset](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip) - Flicker dataset.\n",
    "# * [Flicker8k_Text](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip)\n",
    "\n",
    "\n",
    "# ### Running Locally\n",
    "\n",
    "#root_captioning = \"../Captioning\"\n",
    "\n",
    "\n",
    "# ### Clean/Build Dataset From Flickr8k\n",
    "# \n",
    "# We must pull in the Flickr dataset captions and clean them of extra whitespace, punctuation, and other distractions.\n",
    "\n",
    "null_punct = str.maketrans('', '', string.punctuation)\n",
    "lookup = dict()\n",
    "\n",
    "with open( os.path.join(\"Captioning/Flicker8k_Text\",'Flickr8k.token.txt'), 'r') as fp:\n",
    "  \n",
    "  max_length = 0\n",
    "  for line in fp.read().split('\\n'):\n",
    "    tok = line.split()\n",
    "    if len(line) >= 2:\n",
    "      id = tok[0].split('.')[0]\n",
    "      desc = tok[1:]\n",
    "      \n",
    "      # Cleanup description\n",
    "      desc = [word.lower() for word in desc]\n",
    "      desc = [w.translate(null_punct) for w in desc]\n",
    "      desc = [word for word in desc if len(word)>1]\n",
    "      desc = [word for word in desc if word.isalpha()]\n",
    "      max_length = max(max_length,len(desc))\n",
    "      \n",
    "      if id not in lookup:\n",
    "        lookup[id] = list()\n",
    "      lookup[id].append(' '.join(desc))\n",
    "      \n",
    "lex = set()\n",
    "for key in lookup:\n",
    "  [lex.update(d.split()) for d in lookup[key]]\n",
    "\n",
    "\n",
    "# Stats on what was collected.\n",
    "\n",
    "print(len(lookup)) # How many unique words\n",
    "print(len(lex)) # The dictionary\n",
    "print(max_length) # Maximum length of a caption (in words)\n",
    "\n",
    "\n",
    "# Load the Glove embeddings.\n",
    "# Warning, running this too soon on GDrive can sometimes not work.\n",
    "# Just rerun if len(img) = 0\n",
    "img = glob.glob(os.path.join(root_captioning,'Flicker8k_Dataset', '*.jpg'))\n",
    "\n",
    "\n",
    "# Display the count of how many Glove embeddings we have.\n",
    "len(img)\n",
    "\n",
    "\n",
    "# Read all image names and use the predefined train/test sets.\n",
    "train_images_path = os.path.join(\"Captioning/Flicker8k_Text\",'Flickr8k.token.txt') \n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "test_images_path = os.path.join(\"Captioning/Flicker8k_Text\",'Flickr8k.token.txt') \n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "\n",
    "train_img = []\n",
    "test_img = []\n",
    "\n",
    "for i in img:\n",
    "  f = os.path.split(i)[-1]\n",
    "  if f in train_images: \n",
    "    train_img.append(f) \n",
    "  elif f in test_images:\n",
    "    test_img.append(f) \n",
    "\n",
    "\n",
    "# Display the size of the train and test sets.\n",
    "print(len(train_images))\n",
    "print(len(test_images))\n",
    "\n",
    "\n",
    "# Build the sequences.  We include a **start** and **stop** token at the beginning/end.  We will later use the **start** token to begin the process of generating a caption.  Encountering the **stop** token in the generated text will let us know we are done.\n",
    "train_descriptions = {k:v for k,v in lookup.items() if f'{k}.jpg' in train_images}\n",
    "for n,v in train_descriptions.items(): \n",
    "  for d in range(len(v)):\n",
    "    v[d] = f'{START} {v[d]} {STOP}'\n",
    "\n",
    "\n",
    "# See how many discriptions were extracted.\n",
    "len(train_descriptions)\n",
    "\n",
    "\n",
    "# ### Choosing a Computer Vision Neural Network to Transfer\n",
    "# \n",
    "# There are two neural networks that are accessed via transfer learning.  In this example, I use Glove for the text embedding and InceptionV3 to extract features from the images.  Both of these transfers serve to extract features from the raw text and the images.  Without this prior knowldge transferred in, this example would take consideraby more training.\n",
    "# \n",
    "# I made it so you can interchange the neural network used for the images.  By setting the values WIDTH, HEIGHT, and OUTPUT_DIM you can interchange images.  One characteristic that you are seeking for the image neural network is that it does not have too many outputs (once you strip the 1000-class imagenet classifier, as is common in transfer learning).  InceptionV3 has 2,048 features below the classifier and MobileNet has over 50K.  If the additional dimensions truely capture aspects of the images, then they are worthwhile.  However, having 50K features increases the processing needed and the complexity of the neural network we are constructing.\n",
    "\n",
    "encode_model = InceptionV3(weights='imagenet')\n",
    "encode_model = Model(encode_model.input, encode_model.layers[-2].output)\n",
    "WIDTH = 299\n",
    "HEIGHT = 299\n",
    "OUTPUT_DIM = 2048\n",
    "preprocess_input = tensorflow.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "\n",
    "# The summary for the chosen image neural network to be transfered is displayed.\n",
    "encode_model.summary()\n",
    "\n",
    "\n",
    "# ### Creating the Training Set\n",
    "# \n",
    "# We we need to encode the images to create the training set.  Later we will encode new images to present them for captioning.\n",
    "def encodeImage(img):\n",
    "  # Resize all images to a standard size (specified bythe image encoding network)\n",
    "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)\n",
    "  # Convert a PIL image to a numpy array\n",
    "  x = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "  # Expand to 2D array\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  # Perform any preprocessing needed by InceptionV3 or others\n",
    "  x = preprocess_input(x)\n",
    "  # Call InceptionV3 (or other) to extract the smaller feature set for the image.\n",
    "  x = encode_model.predict(x) # Get the encoding vector for the image\n",
    "  # Shape to correct form to be accepted by LSTM captioning network.\n",
    "  x = np.reshape(x, OUTPUT_DIM )\n",
    "  return x\n",
    "\n",
    "def encodeImageArray(img):\n",
    "  img = tensorflow.keras.preprocessing.image.array_to_img(img)\n",
    "  # Resize all images to a standard size (specified bythe image encoding network)\n",
    "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)\n",
    "  # Convert a PIL image to a numpy array\n",
    "  x = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "  # Expand to 2D array\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  # Perform any preprocessing needed by InceptionV3 or others\n",
    "  x = preprocess_input(x)\n",
    "  # Call InceptionV3 (or other) to extract the smaller feature set for the image.\n",
    "  x = encode_model.predict(x) # Get the encoding vector for the image\n",
    "  # Shape to correct form to be accepted by LSTM captioning network.\n",
    "  x = np.reshape(x, OUTPUT_DIM )\n",
    "  return x\n",
    "\n",
    "\n",
    "# We can how generate the training set.  This will involve looping over every JPG that was provied.  Because this can take awhile to perform we will save it to a pickle file.  This saves the considerable time needed to completly reprocess all of the images.  Because the images are processed differently by different transferred neural networks, the output dimensions are also made part of the file name.  If you changed from InceptionV3 to MobileNet, the number of output dimensions would change, and a new file would be created.\n",
    "train_path = os.path.join(\"Captioning/data\",f'train{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(train_path):\n",
    "  start = time()\n",
    "  encoding_train = {}\n",
    "  for id in tqdm(train_img):\n",
    "    image_path = os.path.join(\"Captioning\",'Flicker8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_train[id] = encodeImage(img)\n",
    "  with open(train_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_train, fp)\n",
    "  print(f\"\\nGenerating training set took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  with open(train_path, \"rb\") as fp:\n",
    "    encoding_train = pickle.load(fp)\n",
    "\n",
    "\n",
    "# A similar process must also be performed for the test images.\n",
    "test_path = os.path.join(\"Captioning/data\",f'test{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(test_path):\n",
    "  start = time()\n",
    "  encoding_test = {}\n",
    "  for id in tqdm(test_img):\n",
    "    image_path = os.path.join(\"Captioning\",'Flicker8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_test[id] = encodeImage(img)\n",
    "  with open(test_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_test, fp)\n",
    "  print(f\"\\nGenerating testing set took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  with open(test_path, \"rb\") as fp:\n",
    "    encoding_test = pickle.load(fp)\n",
    "\n",
    "\n",
    "# Next we separate the captions that will be usef for training.  There are two sides to this training, the images and the captions.\n",
    "\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "len(all_train_captions)\n",
    "\n",
    "\n",
    "# Words that do not occur very often can be misleading to neural network training.  It is better to simply remove such words.  Here we remove any words that occur less than 10 times.  We display what the total vocabulary shrunk to.\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))\n",
    "\n",
    "# Next we build two lookup tables for this vocabulary. One idxtoword convers index numbers to actual words to index values.  The wordtoidx lookup table performs the opposit.\n",
    "\n",
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(idxtoword) + 1 \n",
    "vocab_size\n",
    "\n",
    "\n",
    "# Previously we added a start and stop token to all sentences.  We must account for this in the maximum length of captions.\n",
    "\n",
    "max_length +=2\n",
    "print(max_length)\n",
    "\n",
    "\n",
    "# ### Using a Data Generator\n",
    "# \n",
    "# Up to this point we've always generated training data ahead of time and fit the neural network to it.  It is not always practical to generate all of the training data ahead of time.  The memory demands can be considerable.  If the training data can be generated, as the neural network needs it, it is possable to use a Keras generator.  The generator will create new data, as it is needed.  The generator provided here creates the training data for the caption neural network, as it is needed.\n",
    "# \n",
    "# If we were to build all needed training data ahead of time it would look something like below.\n",
    "# \n",
    "# ![Captioning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/caption-2.png \"Captioning\")\n",
    "# \n",
    "# Here we are just training on two captions.  However, we would have to duplicate the image for each of these partial captions that we have.  Additionally the Flikr8K data set has 5 captions for each picture.  Those would all require duplication of data as well.  It is much more efficient to just generate the data as needed.\n",
    "\n",
    "def data_generator(descriptions, photos, wordtoidx, max_length, num_photos_per_batch):\n",
    "  # x1 - Training data for photos\n",
    "  # x2 - The caption that goes with each photo\n",
    "  # y - The predicted rest of the caption\n",
    "  x1, x2, y = [], [], []\n",
    "  n=0\n",
    "  while True:\n",
    "    for key, desc_list in descriptions.items():\n",
    "      n+=1\n",
    "      photo = photos[key+'.jpg']\n",
    "      # Each photo has 5 descriptions\n",
    "      for desc in desc_list:\n",
    "        # Convert each word into a list of sequences.\n",
    "        seq = [wordtoidx[word] for word in desc.split(' ') if word in wordtoidx]\n",
    "        # Generate a training case for every possible sequence and outcome\n",
    "        for i in range(1, len(seq)):\n",
    "          in_seq, out_seq = seq[:i], seq[i]\n",
    "          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "          x1.append(photo)\n",
    "          x2.append(in_seq)\n",
    "          y.append(out_seq)\n",
    "      if n==num_photos_per_batch:\n",
    "        yield ([np.array(x1), np.array(x2)], np.array(y))\n",
    "        x1, x2, y = [], [], []\n",
    "        n=0\n",
    "\n",
    "\n",
    "# ### Loading Glove Embeddings\n",
    "\n",
    "glove_dir = os.path.join(\"Captioning\",'glove.6B')\n",
    "embeddings_index = {} \n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "\n",
    "# ### Building the Neural Network\n",
    "# \n",
    "# An embedding matrix is built from Glove.  This will be directly copied to the weight matrix of the neural network.\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoidx.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# The matrix makes sense.  It is 1652 (the size of the vocabulary) by 200 (the number of features Glove generates for each word).\n",
    "\n",
    "embedding_matrix.shape\n",
    "\n",
    "inputs1 = Input(shape=(OUTPUT_DIM,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "print(embedding_dim)\n",
    "caption_model.summary()\n",
    "\n",
    "\n",
    "caption_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_model.layers[2].trainable = False\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "start= 0\n",
    "model_path = os.path.join(\"Captioning/data\",f'caption-model.hdf5')\n",
    "caption_model.load_weights(model_path)\n",
    "    \n",
    "\n",
    "\n",
    "# ### Generating Captions\n",
    "# \n",
    "# It is important to understand that a caption is not generated with one single call to the neural network's predict function.  Neural networks output a fixed-length tensor.  To get a variable length output, such as free-form text, requires multiple calls to the neural network.\n",
    "# \n",
    "# The neural network accepts two objects (which are mapped to the input neurons).  The first is the photo.  The second is an ever growing caption.  The caption begins with just the starting token.  The neural network's output is the prediction of the next word in the caption.  This continues until an end token is predicted or we reach the maximum length of a caption.  Each time predict a new word is predicted for the caption.  The word that has the highest probability (from the neural network) is chosen. \n",
    "\n",
    "def generateCaption(photo):\n",
    "    in_text = START\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = caption_model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idxtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == STOP:\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "\n",
    "#@staticmethod\n",
    "def __draw_label(img, text, pos, bg_color):\n",
    "    font_face = cv2.FONT_HERSHEY_TRIPLEX\n",
    "    scale = 0.5\n",
    "    color = (255, 255, 255)\n",
    "    thickness = cv2.FILLED\n",
    "    margin = 5\n",
    "\n",
    "    txt_size = cv2.getTextSize(text, font_face, scale, thickness)\n",
    "\n",
    "    end_x = pos[0] + txt_size[0][0] + margin\n",
    "    end_y = pos[1] - txt_size[0][1] - margin\n",
    "\n",
    "    cv2.rectangle(img, pos, (end_x, end_y), bg_color, thickness)\n",
    "    cv2.putText(img, text, pos, font_face, scale, color, 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "#cap.set(3,640)\n",
    "#cap.set(4,480)\n",
    "\n",
    "if (cap.isOpened()== False):\n",
    "    print(\"Error opening video stream or file\")\n",
    "    \n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    framenc = encodeImageArray(frame).reshape((1,OUTPUT_DIM))\n",
    "    capstr = generateCaption(framenc)\n",
    "    print(\"Caption:\",capstr)\n",
    "    print(\"_____________________________________\")\n",
    "    __draw_label(frame, capstr, (0,150), (50,125,50))\n",
    "    cv2.imshow('Frame',frame)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break     \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
